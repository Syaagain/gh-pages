Search.setIndex({"alltitles": {"Agents": [[4, "agents"], [5, "agents"]], "Contents:": [[0, null]], "Util - Open AI Gym": [[8, "util-open-ai-gym"]], "Welcome to SEPRLHF\u2019s documentation!": [[0, null]], "seprlhf": [[1, null]], "seprlhf.algorithms": [[1, "seprlhf-algorithms"]], "seprlhf.algorithms.ppo": [[2, null]], "seprlhf.algorithms.train_v2 module": [[3, null]], "seprlhf.loss": [[1, "seprlhf-loss"]], "seprlhf.loss.loss": [[4, null]], "seprlhf.models": [[1, "seprlhf-models"]], "seprlhf.models.agents module": [[5, null]], "seprlhf.models.rewards module": [[6, null]], "seprlhf.query_selection": [[1, "seprlhf-query-selection"]], "seprlhf.query_selection.buffer_v2 module": [[7, null]], "seprlhf.utils": [[1, "seprlhf-utils"]], "seprlhf.utils.gym module": [[8, null]], "seprlhf.utils.util module": [[9, null]]}, "docnames": ["index", "modules", "seprlhf.algorithms.ppo", "seprlhf.algorithms.train_v2", "seprlhf.loss.loss", "seprlhf.models.agents", "seprlhf.models.rewards", "seprlhf.query_selection.buffer_v2", "seprlhf.utils.gym", "seprlhf.utils.util"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1}, "filenames": ["index.rst", "modules.rst", "seprlhf.algorithms.ppo.rst", "seprlhf.algorithms.train_v2.rst", "seprlhf.loss.loss.rst", "seprlhf.models.agents.rst", "seprlhf.models.rewards.rst", "seprlhf.query_selection.buffer_v2.rst", "seprlhf.utils.gym.rst", "seprlhf.utils.util.rst"], "indexentries": {"__init__() (seprlhf.models.agents.ppoagent method)": [[5, "seprlhf.models.agents.PPOAgent.__init__", false]], "__init__() (seprlhf.models.rewards.rewardmodel method)": [[6, "seprlhf.models.rewards.RewardModel.__init__", false]], "anneal_lr (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.anneal_lr", false]], "anneal_lr (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.anneal_lr", false]], "anneal_rm_lr (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.anneal_rm_lr", false]], "args (class in seprlhf.algorithms.ppo)": [[2, "seprlhf.algorithms.ppo.Args", false]], "args (class in seprlhf.algorithms.train_v2)": [[3, "seprlhf.algorithms.train_v2.Args", false]], "batch_size (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.batch_size", false]], "batch_size (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.batch_size", false]], "bradley_terry_model() (seprlhf.loss.loss.preferenceloss method)": [[4, "seprlhf.loss.loss.PreferenceLoss.bradley_terry_model", false]], "buffer (class in seprlhf.query_selection.buffer_v2)": [[7, "seprlhf.query_selection.buffer_v2.Buffer", false]], "capture_video (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.capture_video", false]], "capture_video (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.capture_video", false]], "clip_coef (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.clip_coef", false]], "clip_coef (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.clip_coef", false]], "clip_vloss (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.clip_vloss", false]], "clip_vloss (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.clip_vloss", false]], "compare_and_label() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.compare_and_label", false]], "compare_and_label_hf() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.compare_and_label_hf", false]], "compute_mse() (in module seprlhf.utils.util)": [[9, "seprlhf.utils.util.compute_mse", false]], "compute_pearson_correlation() (in module seprlhf.utils.util)": [[9, "seprlhf.utils.util.compute_pearson_correlation", false]], "count_parameters() (in module seprlhf.utils.util)": [[9, "seprlhf.utils.util.count_parameters", false]], "create_reward_models() (in module seprlhf.utils.util)": [[9, "seprlhf.utils.util.create_reward_models", false]], "cuda (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.cuda", false]], "cuda (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.cuda", false]], "delete_videos (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.delete_videos", false]], "dropout_ratio_rm (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.dropout_ratio_rm", false]], "ensamble_size (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.ensamble_size", false]], "ent_coef (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.ent_coef", false]], "ent_coef (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.ent_coef", false]], "env_id (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.env_id", false]], "env_id (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.env_id", false]], "exp_name (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.exp_name", false]], "exp_name (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.exp_name", false]], "forward() (seprlhf.loss.loss.ppoloss method)": [[4, "seprlhf.loss.loss.PPOLoss.forward", false]], "forward() (seprlhf.loss.loss.preferenceloss method)": [[4, "seprlhf.loss.loss.PreferenceLoss.forward", false]], "forward() (seprlhf.models.rewards.rewardmodel method)": [[6, "seprlhf.models.rewards.RewardModel.forward", false]], "gae_lambda (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.gae_lambda", false]], "gae_lambda (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.gae_lambda", false]], "gamma (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.gamma", false]], "gamma (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.gamma", false]], "get_action_and_value() (seprlhf.models.agents.ppoagent method)": [[5, "seprlhf.models.agents.PPOAgent.get_action_and_value", false]], "get_obs_action() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.get_obs_action", false]], "get_value() (seprlhf.models.agents.ppoagent method)": [[5, "seprlhf.models.agents.PPOAgent.get_value", false]], "hf_entity (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.hf_entity", false]], "hf_entity (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.hf_entity", false]], "human_feedback (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.human_feedback", false]], "init_agent (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.init_agent", false]], "init_reward_model (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.init_reward_model", false]], "layer_init() (seprlhf.models.agents.ppoagent method)": [[5, "seprlhf.models.agents.PPOAgent.layer_init", false]], "layer_init() (seprlhf.models.rewards.rewardmodel method)": [[6, "seprlhf.models.rewards.RewardModel.layer_init", false]], "learning_rate (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.learning_rate", false]], "learning_rate (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.learning_rate", false]], "make_gymnasium_env() (in module seprlhf.utils.gym)": [[8, "seprlhf.utils.gym.make_gymnasium_env", false]], "matplotlib (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.matplotlib", false]], "max_episode_steps (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.max_episode_steps", false]], "max_grad_norm (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.max_grad_norm", false]], "max_grad_norm (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.max_grad_norm", false]], "minibatch_size (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.minibatch_size", false]], "minibatch_size (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.minibatch_size", false]], "module": [[2, "module-seprlhf.algorithms.ppo", false], [3, "module-seprlhf.algorithms.train_v2", false], [4, "module-seprlhf.loss.loss", false], [5, "module-seprlhf.models.agents", false], [6, "module-seprlhf.models.rewards", false], [7, "module-seprlhf.query_selection.buffer_v2", false], [8, "module-seprlhf.utils.gym", false], [9, "module-seprlhf.utils.util", false]], "norm_adv (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.norm_adv", false]], "norm_adv (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.norm_adv", false]], "num_envs (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.num_envs", false]], "num_envs (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.num_envs", false]], "num_iterations (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.num_iterations", false]], "num_iterations (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.num_iterations", false]], "num_minibatches (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.num_minibatches", false]], "num_minibatches (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.num_minibatches", false]], "num_preference_pairs (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.num_preference_pairs", false]], "num_segments (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.num_segments", false]], "num_steps (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.num_steps", false]], "num_steps (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.num_steps", false]], "ppoagent (class in seprlhf.models.agents)": [[5, "seprlhf.models.agents.PPOAgent", false]], "ppoloss (class in seprlhf.loss.loss)": [[4, "seprlhf.loss.loss.PPOLoss", false]], "preferenceloss (class in seprlhf.loss.loss)": [[4, "seprlhf.loss.loss.PreferenceLoss", false]], "pretrain (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.pretrain", false]], "pretrain_sample_ratio (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.pretrain_sample_ratio", false]], "random_sample (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.random_sample", false]], "render_actions() (in module seprlhf.utils.gym)": [[8, "seprlhf.utils.gym.render_actions", false]], "reset() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.reset", false]], "reshape() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.reshape", false]], "reward_based_inds() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.reward_based_inds", false]], "reward_based_sample() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.reward_based_sample", false]], "reward_model_hidden_dim (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.reward_model_hidden_dim", false]], "rewardmodel (class in seprlhf.models.rewards)": [[6, "seprlhf.models.rewards.RewardModel", false]], "rm_learning_rate (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.rm_learning_rate", false]], "run_id (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.run_id", false]], "sample() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.sample", false]], "save_model (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.save_model", false]], "save_model (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.save_model", false]], "seed (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.seed", false]], "seed (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.seed", false]], "seprlhf.algorithms.ppo": [[2, "module-seprlhf.algorithms.ppo", false]], "seprlhf.algorithms.train_v2": [[3, "module-seprlhf.algorithms.train_v2", false]], "seprlhf.loss.loss": [[4, "module-seprlhf.loss.loss", false]], "seprlhf.models.agents": [[5, "module-seprlhf.models.agents", false]], "seprlhf.models.rewards": [[6, "module-seprlhf.models.rewards", false]], "seprlhf.query_selection.buffer_v2": [[7, "module-seprlhf.query_selection.buffer_v2", false]], "seprlhf.utils.gym": [[8, "module-seprlhf.utils.gym", false]], "seprlhf.utils.util": [[9, "module-seprlhf.utils.util", false]], "store() (seprlhf.query_selection.buffer_v2.buffer method)": [[7, "seprlhf.query_selection.buffer_v2.Buffer.store", false]], "target_kl (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.target_kl", false]], "target_kl (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.target_kl", false]], "torch_deterministic (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.torch_deterministic", false]], "torch_deterministic (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.torch_deterministic", false]], "total_num_preference_pairs (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.total_num_preference_pairs", false]], "total_timesteps (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.total_timesteps", false]], "total_timesteps (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.total_timesteps", false]], "track (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.track", false]], "track (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.track", false]], "update_epochs (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.update_epochs", false]], "update_epochs (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.update_epochs", false]], "upload_model (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.upload_model", false]], "upload_model (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.upload_model", false]], "validation_ratio (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.validation_ratio", false]], "vf_coef (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.vf_coef", false]], "vf_coef (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.vf_coef", false]], "wandb_entity (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.wandb_entity", false]], "wandb_entity (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.wandb_entity", false]], "wandb_project_name (seprlhf.algorithms.ppo.args attribute)": [[2, "seprlhf.algorithms.ppo.Args.wandb_project_name", false]], "wandb_project_name (seprlhf.algorithms.train_v2.args attribute)": [[3, "seprlhf.algorithms.train_v2.Args.wandb_project_name", false]]}, "objects": {"seprlhf.algorithms": [[2, 0, 0, "-", "ppo"], [3, 0, 0, "-", "train_v2"]], "seprlhf.algorithms.ppo": [[2, 1, 1, "", "Args"]], "seprlhf.algorithms.ppo.Args": [[2, 2, 1, "", "anneal_lr"], [2, 2, 1, "", "batch_size"], [2, 2, 1, "", "capture_video"], [2, 2, 1, "", "clip_coef"], [2, 2, 1, "", "clip_vloss"], [2, 2, 1, "", "cuda"], [2, 2, 1, "", "ent_coef"], [2, 2, 1, "", "env_id"], [2, 2, 1, "", "exp_name"], [2, 2, 1, "", "gae_lambda"], [2, 2, 1, "", "gamma"], [2, 2, 1, "", "hf_entity"], [2, 2, 1, "", "learning_rate"], [2, 2, 1, "", "max_grad_norm"], [2, 2, 1, "", "minibatch_size"], [2, 2, 1, "", "norm_adv"], [2, 2, 1, "", "num_envs"], [2, 2, 1, "", "num_iterations"], [2, 2, 1, "", "num_minibatches"], [2, 2, 1, "", "num_steps"], [2, 2, 1, "", "save_model"], [2, 2, 1, "", "seed"], [2, 2, 1, "", "target_kl"], [2, 2, 1, "", "torch_deterministic"], [2, 2, 1, "", "total_timesteps"], [2, 2, 1, "", "track"], [2, 2, 1, "", "update_epochs"], [2, 2, 1, "", "upload_model"], [2, 2, 1, "", "vf_coef"], [2, 2, 1, "", "wandb_entity"], [2, 2, 1, "", "wandb_project_name"]], "seprlhf.algorithms.train_v2": [[3, 1, 1, "", "Args"]], "seprlhf.algorithms.train_v2.Args": [[3, 2, 1, "", "anneal_lr"], [3, 2, 1, "", "anneal_rm_lr"], [3, 2, 1, "", "batch_size"], [3, 2, 1, "", "capture_video"], [3, 2, 1, "", "clip_coef"], [3, 2, 1, "", "clip_vloss"], [3, 2, 1, "", "cuda"], [3, 2, 1, "", "delete_videos"], [3, 2, 1, "", "dropout_ratio_rm"], [3, 2, 1, "", "ensamble_size"], [3, 2, 1, "", "ent_coef"], [3, 2, 1, "", "env_id"], [3, 2, 1, "", "exp_name"], [3, 2, 1, "", "gae_lambda"], [3, 2, 1, "", "gamma"], [3, 2, 1, "", "hf_entity"], [3, 2, 1, "", "human_feedback"], [3, 2, 1, "", "init_agent"], [3, 2, 1, "", "init_reward_model"], [3, 2, 1, "", "learning_rate"], [3, 2, 1, "", "matplotlib"], [3, 2, 1, "", "max_episode_steps"], [3, 2, 1, "", "max_grad_norm"], [3, 2, 1, "", "minibatch_size"], [3, 2, 1, "", "norm_adv"], [3, 2, 1, "", "num_envs"], [3, 2, 1, "", "num_iterations"], [3, 2, 1, "", "num_minibatches"], [3, 2, 1, "", "num_preference_pairs"], [3, 2, 1, "", "num_segments"], [3, 2, 1, "", "num_steps"], [3, 2, 1, "", "pretrain"], [3, 2, 1, "", "pretrain_sample_ratio"], [3, 2, 1, "", "random_sample"], [3, 2, 1, "", "reward_model_hidden_dim"], [3, 2, 1, "", "rm_learning_rate"], [3, 2, 1, "", "run_id"], [3, 2, 1, "", "save_model"], [3, 2, 1, "", "seed"], [3, 2, 1, "", "target_kl"], [3, 2, 1, "", "torch_deterministic"], [3, 2, 1, "", "total_num_preference_pairs"], [3, 2, 1, "", "total_timesteps"], [3, 2, 1, "", "track"], [3, 2, 1, "", "update_epochs"], [3, 2, 1, "", "upload_model"], [3, 2, 1, "", "validation_ratio"], [3, 2, 1, "", "vf_coef"], [3, 2, 1, "", "wandb_entity"], [3, 2, 1, "", "wandb_project_name"]], "seprlhf.loss": [[4, 0, 0, "-", "loss"]], "seprlhf.loss.loss": [[4, 1, 1, "", "PPOLoss"], [4, 1, 1, "", "PreferenceLoss"]], "seprlhf.loss.loss.PPOLoss": [[4, 3, 1, "", "forward"]], "seprlhf.loss.loss.PreferenceLoss": [[4, 3, 1, "", "bradley_terry_model"], [4, 3, 1, "", "forward"]], "seprlhf.models": [[5, 0, 0, "-", "agents"], [6, 0, 0, "-", "rewards"]], "seprlhf.models.agents": [[5, 1, 1, "", "PPOAgent"]], "seprlhf.models.agents.PPOAgent": [[5, 3, 1, "", "__init__"], [5, 3, 1, "", "get_action_and_value"], [5, 3, 1, "", "get_value"], [5, 3, 1, "", "layer_init"]], "seprlhf.models.rewards": [[6, 1, 1, "", "RewardModel"]], "seprlhf.models.rewards.RewardModel": [[6, 3, 1, "", "__init__"], [6, 3, 1, "", "forward"], [6, 3, 1, "", "layer_init"]], "seprlhf.query_selection": [[7, 0, 0, "-", "buffer_v2"]], "seprlhf.query_selection.buffer_v2": [[7, 1, 1, "", "Buffer"]], "seprlhf.query_selection.buffer_v2.Buffer": [[7, 3, 1, "", "compare_and_label"], [7, 3, 1, "", "compare_and_label_hf"], [7, 3, 1, "", "get_obs_action"], [7, 3, 1, "", "reset"], [7, 3, 1, "", "reshape"], [7, 3, 1, "", "reward_based_inds"], [7, 3, 1, "", "reward_based_sample"], [7, 3, 1, "", "sample"], [7, 3, 1, "", "store"]], "seprlhf.utils": [[8, 0, 0, "-", "gym"], [9, 0, 0, "-", "util"]], "seprlhf.utils.gym": [[8, 4, 1, "", "make_gymnasium_env"], [8, 4, 1, "", "render_actions"]], "seprlhf.utils.util": [[9, 4, 1, "", "compute_mse"], [9, 4, 1, "", "compute_pearson_correlation"], [9, 4, 1, "", "count_parameters"], [9, 4, 1, "", "create_reward_models"]]}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "attribute", "Python attribute"], "3": ["py", "method", "Python method"], "4": ["py", "function", "Python function"]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:attribute", "3": "py:method", "4": "py:function"}, "terms": {"": [2, 3], "0": [2, 3, 4, 5, 6], "0003": [2, 3], "01": [4, 6], "03741": 4, "1": [2, 3, 4, 5, 6, 8], "10": [2, 3], "1000": 3, "1000000": 2, "10000000": 3, "11": [5, 8], "12": 4, "128": 3, "1400": 3, "1706": 4, "2": [2, 3, 4, 5, 6, 8], "20": 3, "2024": [4, 5, 8], "2048": [2, 3], "29": [5, 8], "3": [3, 4, 6], "30": 8, "32": [2, 3], "4": 3, "4142135623730951": [5, 6], "5": [2, 3, 4], "64": [3, 4, 6], "95": [2, 3], "99": [2, 3], "A": 7, "For": [4, 5, 8], "If": [4, 7], "The": [4, 6, 7], "__init__": [1, 5, 6], "_description_": 5, "_type_": [5, 9], "act_dim": 7, "action": [4, 5, 6, 7, 8], "action_shap": 7, "actions_tensor": 7, "activ": 6, "actual": 6, "adam": 9, "adjust": 6, "adujust": 6, "advantag": [2, 3, 4], "after": [3, 6, 7], "afterward": 8, "agent": [0, 1, 2, 3, 6, 7], "ai": [0, 1], "al": [4, 6], "algorithm": 0, "all": 4, "along": [4, 6], "alpha": 6, "alreadi": 7, "also": 4, "alwai": 4, "an": [6, 8, 9], "anneal": [2, 3], "anneal_lr": [1, 2, 3], "anneal_rm_lr": [1, 3], "appli": [4, 6], "approach": 4, "ar": [4, 7], "architectur": 6, "arg": [0, 1, 2, 3, 9], "argpars": 9, "arr": 7, "arrai": 7, "arxiv": 4, "ascend": 7, "assign": [4, 7], "automat": 3, "avail": 4, "averag": 4, "backend": [2, 3], "bad": 4, "base": [2, 3, 4, 5, 6, 7, 8], "batch": [2, 3, 4, 6, 7], "batch_1": 3, "batch_siz": [1, 2, 3, 4], "becom": 4, "befor": 4, "better": 6, "between": [4, 9], "bia": [5, 6], "bias": [2, 3], "bias_const": [5, 6], "bonu": 4, "bool": [2, 3, 4, 5, 6, 7, 8], "bradley_terry_model": [1, 4], "buffer": [0, 1, 3, 7], "buffer_v2": [0, 1], "c": [4, 5, 8], "calcul": [4, 7], "calucl": 4, "can": [4, 6, 7], "capabl": 4, "captur": [2, 3, 8], "capture_video": [1, 2, 3, 8], "cartpol": 8, "chang": 4, "check": [2, 3], "christiano": [4, 6], "class": [2, 3, 4, 5, 6, 7], "cleanrl": [2, 3], "clip": [2, 3, 4], "clip_coef": [1, 2, 3, 4], "clip_vloss": [1, 2, 3, 4], "coeffici": [2, 3, 4], "collect": 3, "compar": [7, 8], "compare_and_label": [1, 7], "compare_and_label_hf": [1, 7], "comput": [2, 3, 4, 9], "compute_ga": 4, "compute_ms": [0, 1, 9], "compute_pearson_correl": [0, 1, 9], "concaten": 6, "concurr": 3, "contain": [4, 7], "containin": 4, "copi": [4, 5, 8], "copmut": 4, "copyright": [4, 5, 8], "core": 4, "correl": 9, "correspond": 7, "count": 9, "count_paramet": [0, 1, 9], "cours": 4, "cpu": 7, "creat": [4, 5, 8, 9], "create_reward_model": [0, 1, 9], "criteria": 7, "critic": 5, "cross": 4, "cuda": [1, 2, 3, 7], "cudnn": [2, 3], "cumul": 7, "current": [4, 6, 7], "data": 7, "dataset": 4, "deal": 4, "decid": 3, "deep": [4, 6], "deeper": 6, "default": [2, 3, 4, 5, 6, 8], "defualt": 4, "delet": [3, 7], "delete_video": [1, 3, 7], "detail": [4, 5, 8], "determinist": [2, 3, 4], "deviat": 4, "devic": [4, 7, 9], "dict": 7, "dictionari": 7, "differ": [5, 6], "dimens": [3, 6], "dimension": 7, "directori": 7, "discount": [2, 3, 4, 8], "distribut": [4, 5], "diverg": [2, 3], "document": 2, "done": 7, "dropout": 3, "dropout_r": 6, "dropout_ratio": 6, "dropout_ratio_rm": [1, 3], "droput": 6, "dure": 4, "e": [7, 8], "each": [2, 3, 4, 7, 9], "easier": 7, "effici": 4, "enabl": [2, 3, 6], "ensambl": [4, 9], "ensamble_s": [1, 3], "ensur": 4, "ent_coef": [1, 2, 3, 4], "entiti": [2, 3], "entropi": [2, 3, 4, 7], "entropy_bonu": 4, "entropy_loss": 4, "env": [4, 5, 6, 8, 9], "env_id": [1, 2, 3, 8], "env_nam": 8, "environ": [2, 3, 5, 6, 7, 8, 9], "episod": [3, 7], "epoch": [2, 3], "error": 9, "estim": [2, 3, 4, 7], "et": [4, 6], "everi": 8, "exampl": 4, "exp_nam": [1, 2, 3], "experi": [2, 3], "explor": 4, "extract": 7, "face": [2, 3], "factor": [2, 3, 4, 8], "fals": [2, 3, 5, 6], "featur": [4, 6], "feedback": 3, "feedforward": 6, "file": [4, 5, 7, 8], "final": 4, "first": [3, 4, 6, 7], "fit": 4, "flag": 7, "flatten": 9, "float": [2, 3, 4, 5, 6, 8, 9], "float64": [5, 6], "folder": [2, 3], "follow": [4, 6], "forward": [1, 4, 6], "fp": 8, "frame": [4, 8], "from": [2, 3, 4, 6, 7, 9], "function": [2, 3, 4, 6, 7, 8], "futur": 4, "g": [7, 8], "gae": 4, "gae_lambda": [1, 2, 3], "game": [2, 3], "gamma": [1, 2, 3, 8], "gener": [2, 3, 4, 7], "get": [4, 6], "get_action_and_valu": [1, 5], "get_obs_act": [1, 7], "get_valu": [1, 5], "give": [5, 6], "given": [4, 5, 6, 9], "gpu": 4, "gradient": [2, 3, 4], "ground": 9, "gt_rewards_flat": 9, "gym": [0, 1, 5, 6, 9], "gymnasium": 8, "gymnasiumd": 5, "ha": 7, "halfcheetah": [2, 3], "handl": 4, "happen": 4, "have": 4, "help": 4, "here": 6, "hf_entiti": [1, 2, 3], "hidden": [3, 6], "hidden_dim": 6, "higher": [4, 6, 7], "how": [3, 4], "http": 4, "hub": [2, 3], "hug": [2, 3], "huggingfac": [2, 3], "human": [3, 4, 6], "human_feedback": [1, 3], "i": [2, 4, 6, 9], "id": [2, 3, 8], "idx": 7, "idx_list": 7, "implement": 4, "implment": 6, "improv": 4, "incentiv": 4, "includ": 7, "increas": 4, "indic": 7, "individu": [3, 6, 9], "influenc": 4, "init": 5, "init_ag": [1, 3], "init_lay": [5, 6], "init_reward_model": [1, 3], "initalis": 4, "initi": [3, 5, 6, 7], "input": [4, 5, 6, 7], "int": [2, 3, 4, 6, 7, 8, 9], "intern": 7, "iter": [2, 3], "k": [2, 3], "keep": 6, "kl": [2, 3], "label": [4, 7], "lambda": [2, 3], "larg": 4, "last": 6, "layer": [3, 5, 6], "layer_init": [1, 5, 6], "leaki": 6, "leaky_alpha": 6, "learn": [2, 3, 4, 6, 9], "learning_r": [1, 2, 3, 9], "licens": [4, 5, 8], "like": [4, 5, 7], "linear": [5, 6], "list": [4, 7, 8, 9], "log": [4, 7], "logprob": 7, "logratio": 4, "loss": [0, 2, 3], "lower": 7, "make_gymnasium_env": [0, 1, 8], "mani": [3, 4], "manner": 4, "match": 4, "matplotlib": [1, 3], "max": [4, 5], "max_episode_step": [1, 3], "max_grad_norm": [1, 2, 3], "max_siz": 7, "maximum": [2, 3, 7], "mean": [4, 6, 9], "measur": 4, "method": 5, "migat": 4, "mini": [2, 3], "minibatch_s": [1, 2, 3], "model": [0, 2, 3, 4, 9], "modul": [0, 1, 4], "more": [4, 5, 6], "mse": 9, "mujoco": 6, "multi": 7, "multipl": 4, "n_env": 7, "name": [2, 3, 8], "ndarrai": 7, "need": [4, 6], "network": [2, 3, 6, 9], "neural": [3, 6, 9], "new": [4, 7], "newlogprob": 4, "newvalu": 4, "next": 7, "next_ob": 7, "nn": [4, 5, 6, 9], "none": [2, 3, 5, 7], "norm": [2, 3], "norm_adv": [1, 2, 3, 4], "normal": [2, 3, 4, 8], "note": 4, "np": [5, 6, 7], "num_env": [1, 2, 3, 8], "num_iter": [1, 2, 3], "num_minibatch": [1, 2, 3], "num_model": 9, "num_preference_pair": [1, 3, 7], "num_seg": [1, 3, 7], "num_step": [1, 2, 3], "number": [2, 3, 4, 6, 7, 8, 9], "ob": 7, "object": [2, 3, 7], "obs_action_1": 4, "obs_action_2": 4, "obs_dim": 7, "obs_shap": 7, "obs_tensor": 7, "observ": [4, 5, 6, 7], "old": 4, "oldlogprob": 4, "oldvalu": 4, "one": [4, 6], "onli": 4, "open": [0, 1], "openai": 8, "optim": [2, 3, 4, 6, 9], "option": [4, 5, 6, 7, 8], "order": 7, "org": [2, 3, 4], "other": [4, 6, 7], "out": [2, 3], "output": 8, "over": 4, "pack": 4, "pair": [3, 4, 6, 7], "paper": [2, 3, 6], "parallel": [2, 3, 7, 8], "paramet": [4, 5, 6, 7, 8, 9], "paramt": 9, "parent": 7, "pass": 4, "passe": 9, "path": [7, 8], "pdf": 4, "pearson": 9, "penal": 4, "per": [2, 3, 8], "perform": [2, 3], "place": 9, "pleas": [4, 5, 8], "plot": 3, "plu": 4, "point": 6, "polic": 4, "polici": [2, 3, 4], "policy_loss": 4, "polii": 4, "ppo": [0, 1, 3, 4, 5, 6], "ppoagent": [0, 1, 5], "ppoloss": [0, 1, 4], "prediciont": 4, "predict": [4, 6], "pref_loss": 4, "prefer": [3, 4, 6, 7], "preference_pair": 4, "preferenceloss": [0, 1, 4], "prefernc": 4, "preffer": 4, "prepar": 7, "present": 6, "pretra": 3, "pretrain": [1, 3], "pretrain_sample_ratio": [1, 3], "prevent": 4, "previou": [4, 6], "print": 9, "prob": 4, "probabl": [4, 5, 7], "process": [4, 7], "product": 6, "project": [2, 3], "provid": [6, 7], "proxim": [2, 4], "py": 4, "pytorch": [7, 9], "query_select": 0, "random": [4, 7], "random_sampl": [1, 3], "random_shuffl": 4, "rank": 7, "rate": [2, 3, 9], "ratio": [3, 4, 6], "receiv": 7, "refer": [4, 5, 8], "reinforc": [4, 6], "relev": 7, "relu": 6, "render": 8, "render_act": [0, 1, 8], "repositori": [2, 3], "repres": 5, "reset": [1, 7], "reshap": [1, 7], "reson": [5, 6], "respect": 4, "respet": 6, "retriev": 7, "retrun": 4, "return": [4, 5, 6, 7, 8, 9], "reward": [0, 1, 3, 4, 7, 8, 9], "reward_based_ind": [1, 7], "reward_based_sampl": [1, 7], "reward_model": 4, "reward_model1": 4, "reward_model2": 4, "reward_model_hidden_dim": [1, 3], "rewardmodel": [0, 1, 4, 6], "rl": 5, "rm_learning_r": [1, 3], "rm_rewards_flat": 9, "rollout": [2, 3], "run": [2, 3, 4, 8], "run_id": [1, 3], "run_nam": [2, 3, 8], "runtim": [2, 3], "same": [3, 4, 6], "sampl": [1, 3, 7], "save": [2, 3, 8], "save_model": [1, 2, 3], "second": [3, 4, 6, 7, 8, 9], "section": 4, "see": 4, "seed": [1, 2, 3], "seg": 7, "seg_1": 7, "seg_2": 7, "seg_nr": 8, "segment": [3, 4, 7, 8], "select": [4, 7], "sequenti": 7, "set": 4, "shape": [4, 7], "should": [3, 4, 5, 6, 8, 9], "single_action_space_shap": 6, "single_observations_shap": 6, "singular": 6, "size": [2, 3, 4, 6], "softmax": 4, "sort": 7, "sourc": [2, 3, 4, 5, 6, 7, 8, 9], "space": 7, "specif": [5, 6], "specifi": [4, 7, 8], "specifii": 3, "speed": 4, "sqrt": [5, 6], "squar": 9, "stack": 4, "standard": [4, 6], "start": 6, "state": [4, 6, 7], "std": [5, 6], "step": [2, 3], "storag": 7, "store": [1, 7, 9], "str": [2, 3, 7, 8], "suffl": 4, "sum": 4, "sure": 6, "surrog": [2, 3], "syncvectorenv": [5, 6, 8], "synthet": [3, 4], "take": [6, 7, 9], "taken": [4, 6, 7], "target": [2, 3, 4], "target_kl": [1, 2, 3], "task": 5, "team": [2, 3], "tensor": [4, 5, 6, 7, 8, 9], "tensord": 6, "termin": 7, "them": [4, 5, 6, 7, 8], "thi": [2, 3, 4, 5, 6, 7], "threshold": [2, 3], "time": [4, 6], "timestep": [2, 3], "tobia": [4, 5, 8], "toggl": [2, 3], "too": 4, "torch": [2, 3, 4, 5, 6, 7, 8, 9], "torch_determinist": [1, 2, 3], "total": [2, 3, 4, 9], "total_num_preference_pair": [1, 3], "total_timestep": [1, 2, 3], "track": [1, 2, 3], "train": [4, 6, 8], "train_v2": [0, 1], "trainabl": [6, 9], "trajectori": 4, "trajectory1": 4, "trajectory2": 4, "trajectory_length": 7, "transit": [3, 7], "true": [2, 3, 4], "truth": 9, "tupl": [3, 4, 5, 6, 7, 9], "two": [6, 7], "type": [4, 5, 6, 7, 8, 9], "under": 4, "union": 4, "uniqu": 8, "up": 4, "updat": [2, 3, 4], "update_epoch": [1, 2, 3], "upload": [2, 3], "upload_model": [1, 2, 3], "us": [2, 3, 4, 6, 7, 8], "user": [2, 3, 7], "usual": [4, 5], "util": [0, 4], "v1": 8, "v4": [2, 3], "validation_ratio": [1, 3], "valu": [2, 3, 4, 5, 6, 7], "value_estim": 4, "value_loss": 4, "vari": 4, "variabl": 6, "variou": 4, "vector": [5, 6, 8, 9], "versa": 4, "vf_coef": [1, 2, 3, 4], "vice": 4, "video": [2, 3, 7, 8], "video_dir": 8, "video_loading_dir": 7, "view": 4, "wa": 3, "wandb": [2, 3], "wandb_ent": [1, 2, 3], "wandb_project_nam": [1, 2, 3], "weight": [2, 3, 4, 5, 6], "well": [4, 6], "wether": 4, "when": 4, "where": 9, "whether": [2, 3, 5, 7, 8], "which": [3, 4, 7], "while": [6, 8], "within": 4, "witt": [4, 5, 8], "would": 4, "x": 5, "yield": 7, "you": 4}, "titles": ["Welcome to SEPRLHF\u2019s documentation!", "seprlhf", "seprlhf.algorithms.ppo", "seprlhf.algorithms.train_v2 module", "seprlhf.loss.loss", "seprlhf.models.agents module", "seprlhf.models.rewards module", "seprlhf.query_selection.buffer_v2 module", "seprlhf.utils.gym module", "seprlhf.utils.util module"], "titleterms": {"": 0, "agent": [4, 5], "ai": 8, "algorithm": [1, 2, 3], "buffer_v2": 7, "content": 0, "document": 0, "gym": 8, "loss": [1, 4], "model": [1, 5, 6], "modul": [3, 5, 6, 7, 8, 9], "open": 8, "ppo": 2, "query_select": [1, 7], "reward": 6, "seprlhf": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], "train_v2": 3, "util": [1, 8, 9], "welcom": 0}})